{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embedding-Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZiGdUMZTcJGYPlUEvWHDT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stphnhng/482Project2/blob/master/Embedding_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lVjNK8shFKOC",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# Install the latest Tensorflow version.\n",
        "!pip3 install --upgrade tensorflow-gpu\n",
        "# Install TF-Hub.\n",
        "!pip3 install tensorflow-hub\n",
        "!pip3 install seaborn\n",
        "\n",
        "#!mkdir encoder\n",
        "#!curl -Lo encoder/infersent1.pkl https://dl.fbaipublicfiles.com/infersent/infersent1.pkl\n",
        "#!curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zwty8Z6mAkdV",
        "outputId": "0d7ddf80-372c-47ba-d05c-cb6f83cc303a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Load the Universal Sentence Encoder's TF Hub module\n",
        "from absl import logging\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieTu-Q3yfNKc",
        "colab_type": "code",
        "outputId": "14bdbda8-7b15-4c65-a6fd-b3a1d6402ee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqB_2id4gDE7",
        "colab_type": "code",
        "outputId": "1c95b78c-635a-446e-e495-be34f8f34e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 1.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp36-none-any.whl size=828255078 sha256=89978bd10d6f8ca0995941c771919c90fe63402b332e07b936adf5a8228a2e91\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yejferr4/wheels/b4/d7/70/426d313a459f82ed5e06cc36a50e2bb2f0ec5cb31d8e0bdf09\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfD0xxongHEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzQcEW3TnFpT",
        "colab_type": "code",
        "outputId": "e86c43e5-95f4-494b-a460-e135ef6d2de4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "suEn7bSQebWp",
        "colab": {}
      },
      "source": [
        "def read_json(file):\n",
        "  with open(file, 'r') as fp:\n",
        "    return json.loads(fp.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gGEkKEoBebWv",
        "colab": {}
      },
      "source": [
        "# Read in the data\n",
        "data = read_json(\"/content/drive/My Drive/data_wInfo.json\")\n",
        "qa = read_json(\"/content/drive/My Drive/cp_alexa_qa.json\")\n",
        "generated_qa = read_json(\"/content/drive/My Drive/generated_qa.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5t31uEmfY6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_8vVS3fRBzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_content(data):\n",
        "  sentences = []\n",
        "  outputs = []\n",
        "  for section, content in data.items():\n",
        "    if content['content'] != '':\n",
        "      sent_toke = sent_tokenize(content['content'])\n",
        "      for index, sent in enumerate(sent_toke):\n",
        "        if sent == 'The W.K.':\n",
        "          sent_toke[index+1] = sent + sent_toke[index+1]\n",
        "        elif sent == 'Alumni include Abel Maldonado, Former California Lt.':\n",
        "          sent_toke[index+1] = sent + sent_toke[index+1]\n",
        "        else:\n",
        "          word_list = nltk.word_tokenize(sent)\n",
        "          output = [w for w in word_list if not w in stop_words]\n",
        "          outputs.append(' '.join(output))\n",
        "          sentences.append(sent)\n",
        "  return sentences, outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GSBph_kXd3-3",
        "colab": {}
      },
      "source": [
        "sentences, outputs = parse_content(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chd-z2BYk0vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for question in generated_qa:\n",
        "    answer = generated_qa[question]\n",
        "    if answer not in sentences:\n",
        "        sentences.append(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Rh7HRljd3-6",
        "colab": {}
      },
      "source": [
        "sentences_embeddings = embed(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7flqVMjGd3-9",
        "colab": {}
      },
      "source": [
        "outputs_embeddings = embed(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V0wfD4MId3-_",
        "colab": {}
      },
      "source": [
        "def cos_sim(A, B):\n",
        "  return np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snkcORIGlf3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match_generated(query):\n",
        "    qa_generated_qs = list(generated_qa.items())\n",
        "    most_similar_ans = \"\"\n",
        "    most_similar_num = 0.0\n",
        "    for q,answer in qa_generated_qs:\n",
        "        removed_stopwords = [word for word in nltk.word_tokenize(q) if word not in stop_words]\n",
        "        q = ' '.join(removed_stopwords)\n",
        "        sim = nlp(query).similarity(nlp(q))\n",
        "        if sim >= 0.75 and sim > most_similar_num:\n",
        "            most_similar_ans = answer\n",
        "            most_similar_num = sim\n",
        "    print(\"returning match_generated:\", most_similar_ans)\n",
        "    print(\"\\tgenerated_sim:\", most_similar_num)\n",
        "    return most_similar_ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3DUbvO6nihg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match_qa(query):\n",
        "    qa_infobox_qs = list(qa.items())[:68] # >68 are infobox questions\n",
        "    most_similar_ans = \"\"\n",
        "    most_similar_num = 0.0\n",
        "    for q,answer in qa_infobox_qs:\n",
        "        removed_stopwords = [word for word in nltk.word_tokenize(q) if word not in stop_words]\n",
        "        q = ' '.join(removed_stopwords)\n",
        "        sim = nlp(query).similarity(nlp(q))\n",
        "        if sim >= 0.75 and sim > most_similar_num:\n",
        "            most_similar_ans = answer\n",
        "            most_similar_num = sim\n",
        "    print(\"returning match_qa:\", most_similar_ans)\n",
        "    print(\"\\tqa sim:\", most_similar_num)\n",
        "    return most_similar_ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDi6OpVMfcPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match_infobox(query):\n",
        "    qa_infobox_qs = list(qa.items())[68:] # >68 are infobox questions\n",
        "    most_similar_ans = \"\"\n",
        "    most_similar_num = 0.0\n",
        "    for q,answer in qa_infobox_qs:\n",
        "        removed_stopwords = [word for word in nltk.word_tokenize(q) if word not in stop_words]\n",
        "        q = ' '.join(removed_stopwords)\n",
        "        sim = nlp(query).similarity(nlp(q))\n",
        "        if sim >= 0.75 and sim > most_similar_num:\n",
        "            most_similar_ans = answer\n",
        "            most_similar_num = sim\n",
        "    print(\"returning match_infobox:\", most_similar_ans)\n",
        "    print(\"\\tinfobox sim:\", most_similar_num)\n",
        "    return most_similar_ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QnsPpSZ0d3_E",
        "colab": {}
      },
      "source": [
        "def find_sent_index(query, sentences_embeddings):\n",
        "  query_embedding = embed([query])\n",
        "  max_sim_index = 0\n",
        "  max_sim = -1\n",
        "  sim_list = []\n",
        "  for index, embedding in enumerate(sentences_embeddings):\n",
        "    current_sim = cos_sim(query_embedding, embedding)[0]\n",
        "    sim_list.append((index, current_sim))\n",
        "    if current_sim > max_sim:\n",
        "      max_sim = current_sim\n",
        "      max_sim_index = index\n",
        "  sim_list.sort(key=lambda x: x[1], reverse=True)\n",
        "#   if max_sim < 0.5:\n",
        "  info_q = match_infobox(query)\n",
        "  generated_q = match_generated(query)\n",
        "  qa_q = match_qa(query)\n",
        "  print(\"embedding sim:\",max_sim)\n",
        "  return max_sim_index, sim_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6dKxJfPeSFx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "0ad75cbb-da11-4f66-b6b8-d94113d40a82"
      },
      "source": [
        "index, scores = find_sent_index(\"?\", sentences_embeddings)\n",
        "if index == -1:\n",
        "    print(scores)\n",
        "else:\n",
        "    print(\"embedding:\", sentences[index])"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "returning match_infobox: 1,659 (Fall 2018)\n",
            "\tinfobox sim: 0.8270330682096495\n",
            "returning match_generated: Cal Poly received one of the largest gifts towards public education to be received in California on May 3,2017.\n",
            "\tgenerated_sim: 0.8483933707183999\n",
            "returning match_qa: In 1960, control of Cal Poly San Luis Obispo and all other state colleges was transferred from the State Board of Education to an independent Board of Trustees, which later became the California State University system.\n",
            "\tqa sim: 0.8454570191571539\n",
            "embedding sim: 0.47287977\n",
            "embedding: The school is typically referred to as \"Cal Poly\", or simply \"Poly\".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqKMuK5Bjlih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}